'use client';

import React, { useRef, useEffect, useState, useCallback } from 'react';
import { Card, CardHeader, CardTitle, CardContent } from '../ui/Card';
import { ActionButton } from '../ui/ActionButton';
import { 
  Camera, 
  Mic, 
  MicOff, 
  Video, 
  VideoOff, 
  Phone, 
  PhoneOff,
  Settings,
  Maximize2,
  Minimize2,
  MessageSquare,
  Brain,
  Activity
} from 'lucide-react';
import { VADScore, EmotionAnalysis } from '../../types';
import { emotionEmojis } from '../../utils/emotion';

interface VideoCallEmotionAnalysisProps {
  onEmotionChange?: (emotion: EmotionAnalysis) => void;
  onCallEnd?: () => void;
  isActive?: boolean;
}

export default function VideoCallEmotionAnalysis({ 
  onEmotionChange, 
  onCallEnd,
  isActive = false 
}: VideoCallEmotionAnalysisProps) {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  
  // ìƒíƒœ ê´€ë¦¬
  const [isVideoOn, setIsVideoOn] = useState(true);
  const [isAudioOn, setIsAudioOn] = useState(true);
  const [isAnalyzing, setIsAnalyzing] = useState(false);
  const [isFullscreen, setIsFullscreen] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState<EmotionAnalysis | null>(null);
  const [error, setError] = useState<string | null>(null);
  const [stream, setStream] = useState<MediaStream | null>(null);
  
  // ì‹¤ì‹œê°„ ë¶„ì„ ìƒíƒœ
  const [facialVAD, setFacialVAD] = useState<VADScore>({ valence: 0.5, arousal: 0.5, dominance: 0.5 });
  const [voiceVAD, setVoiceVAD] = useState<VADScore>({ valence: 0.5, arousal: 0.5, dominance: 0.5 });
  const [confidence, setConfidence] = useState(0);

  const animationFrameRef = useRef<number | null>(null);

  // ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™”
  const initializeMediaStream = useCallback(async () => {
    try {
      const mediaStream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: 'user'
        },
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      setStream(mediaStream);
      
      if (videoRef.current) {
        videoRef.current.srcObject = mediaStream;
      }

      // ì˜¤ë””ì˜¤ ë¶„ì„ ì´ˆê¸°í™”
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      }

      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 2048;
      analyserRef.current.smoothingTimeConstant = 0.8;

      microphoneRef.current = audioContextRef.current.createMediaStreamSource(mediaStream);
      microphoneRef.current.connect(analyserRef.current);

      return true;
    } catch (err) {
      console.error('ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì‹¤íŒ¨:', err);
      setError('ì¹´ë©”ë¼ë‚˜ ë§ˆì´í¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      return false;
    }
  }, []);

  // ì‹¤ì‹œê°„ ê°ì • ë¶„ì„
  const analyzeEmotion = useCallback(() => {
    if (!isAnalyzing) return;

    let currentVoiceVAD: VADScore = { valence: 0.5, arousal: 0.5, dominance: 0.5 };
    let currentFacialVAD: VADScore = { valence: 0.5, arousal: 0.5, dominance: 0.5 };

    // ìŒì„± ë¶„ì„
    if (analyserRef.current && isAudioOn) {
      const bufferLength = analyserRef.current.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      const timeDataArray = new Uint8Array(bufferLength);

      analyserRef.current.getByteFrequencyData(dataArray);
      analyserRef.current.getByteTimeDomainData(timeDataArray);

      const volumeLevel = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength / 255;
      const pitchValue = calculatePitch(timeDataArray);
      const rateValue = calculateSpeechRate(timeDataArray);

      currentVoiceVAD = calculateVoiceVAD(volumeLevel, pitchValue, rateValue);
    }

    // í‘œì • ë¶„ì„ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)
    if (isVideoOn) {
      currentFacialVAD = calculateFacialVAD();
    }

    // í†µí•© VAD ì ìˆ˜ ê³„ì‚°
    const integratedVAD = calculateIntegratedVAD(currentFacialVAD, currentVoiceVAD);
    const emotion = vadToEmotion(integratedVAD);
    const confidenceScore = calculateConfidence(currentFacialVAD, currentVoiceVAD);

    const emotionAnalysis: EmotionAnalysis = {
      id: `realtime_${Date.now()}`,
      userId: 'user123',
      timestamp: new Date().toISOString(),
      vadScore: integratedVAD,
      emotion,
      confidence: confidenceScore,
      mediaType: 'realtime',
      cbtFeedback: {
        cognitiveDistortion: 'ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘',
        challenge: 'í˜„ì¬ ê°ì • ìƒíƒœë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”',
        alternative: 'ê°ì • ë³€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°›ì•„ë“¤ì´ì„¸ìš”',
        actionPlan: 'ì •ê¸°ì ì¸ ê°ì • ì²´í¬ë¥¼ í•´ë³´ì„¸ìš”'
      }
    };

    // ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
    setVoiceVAD(currentVoiceVAD);
    setFacialVAD(currentFacialVAD);
    setCurrentEmotion(emotionAnalysis);
    setConfidence(confidenceScore);

    if (onEmotionChange) {
      onEmotionChange(emotionAnalysis);
    }

    animationFrameRef.current = requestAnimationFrame(analyzeEmotion);
  }, [isAnalyzing, isAudioOn, isVideoOn, onEmotionChange]);

  // í”¼ì¹˜ ê³„ì‚°
  const calculatePitch = (timeData: Uint8Array): number => {
    let sum = 0;
    let count = 0;
    
    for (let i = 0; i < timeData.length; i++) {
      const value = (timeData[i] - 128) / 128;
      sum += Math.abs(value);
      count++;
    }
    
    return count > 0 ? sum / count : 0;
  };

  // ìŒì„± ì†ë„ ê³„ì‚°
  const calculateSpeechRate = (timeData: Uint8Array): number => {
    let zeroCrossings = 0;
    
    for (let i = 1; i < timeData.length; i++) {
      const prev = (timeData[i - 1] - 128) / 128;
      const curr = (timeData[i] - 128) / 128;
      
      if ((prev < 0 && curr >= 0) || (prev >= 0 && curr < 0)) {
        zeroCrossings++;
      }
    }
    
    return zeroCrossings / timeData.length;
  };

  // ìŒì„± VAD ê³„ì‚°
  const calculateVoiceVAD = (volume: number, pitch: number, rate: number): VADScore => {
    return {
      valence: Math.min(1, Math.max(0, 0.5 + (pitch - 0.5) * 0.3)),
      arousal: Math.min(1, Math.max(0, volume * 0.8 + rate * 0.2)),
      dominance: Math.min(1, Math.max(0, 0.5 + (volume - 0.5) * 0.4))
    };
  };

  // í‘œì • VAD ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)
  const calculateFacialVAD = (): VADScore => {
    // ì‹¤ì œë¡œëŠ” MediaPipe Face Meshë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
    return {
      valence: 0.5 + (Math.random() - 0.5) * 0.2,
      arousal: 0.5 + (Math.random() - 0.5) * 0.2,
      dominance: 0.5 + (Math.random() - 0.5) * 0.2
    };
  };

  // í†µí•© VAD ê³„ì‚°
  const calculateIntegratedVAD = (facial: VADScore, voice: VADScore): VADScore => {
    return {
      valence: (facial.valence * 0.6 + voice.valence * 0.4),
      arousal: (facial.arousal * 0.4 + voice.arousal * 0.6),
      dominance: (facial.dominance * 0.5 + voice.dominance * 0.5)
    };
  };

  // VADë¥¼ ê°ì •ìœ¼ë¡œ ë³€í™˜
  const vadToEmotion = (vad: VADScore): string => {
    const { valence, arousal, dominance } = vad;
    
    if (valence > 0.7 && arousal > 0.7) return 'excited';
    if (valence > 0.7 && arousal < 0.3) return 'calm';
    if (valence > 0.6) return 'happy';
    if (valence < 0.4 && arousal > 0.7) return 'angry';
    if (valence < 0.4 && arousal < 0.3) return 'sad';
    if (dominance < 0.4 && arousal > 0.6) return 'anxious';
    
    return 'neutral';
  };

  // ì‹ ë¢°ë„ ê³„ì‚°
  const calculateConfidence = (facial: VADScore, voice: VADScore): number => {
    const facialConfidence = 0.8; // ì‹¤ì œë¡œëŠ” ì–¼êµ´ ì¸ì‹ ì‹ ë¢°ë„
    const voiceConfidence = 0.7; // ì‹¤ì œë¡œëŠ” ìŒì„± í’ˆì§ˆ ì‹ ë¢°ë„
    return (facialConfidence * 0.6 + voiceConfidence * 0.4);
  };

  // ë¶„ì„ ì‹œì‘/ì¤‘ì§€
  const toggleAnalysis = useCallback(() => {
    setIsAnalyzing(prev => !prev);
  }, []);

  // ë¹„ë””ì˜¤ í† ê¸€
  const toggleVideo = useCallback(() => {
    if (stream) {
      const videoTrack = stream.getVideoTracks()[0];
      if (videoTrack) {
        videoTrack.enabled = !videoTrack.enabled;
        setIsVideoOn(videoTrack.enabled);
      }
    }
  }, [stream]);

  // ì˜¤ë””ì˜¤ í† ê¸€
  const toggleAudio = useCallback(() => {
    if (stream) {
      const audioTrack = stream.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !audioTrack.enabled;
        setIsAudioOn(audioTrack.enabled);
      }
    }
  }, [stream]);

  // ì „ì²´í™”ë©´ í† ê¸€
  const toggleFullscreen = useCallback(() => {
    if (!document.fullscreenElement) {
      videoRef.current?.requestFullscreen();
      setIsFullscreen(true);
    } else {
      document.exitFullscreen();
      setIsFullscreen(false);
    }
  }, []);

  // í†µí™” ì¢…ë£Œ
  const endCall = useCallback(() => {
    if (stream) {
      stream.getTracks().forEach(track => track.stop());
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
    if (onCallEnd) {
      onCallEnd();
    }
  }, [stream, onCallEnd]);

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ì´ˆê¸°í™”
  useEffect(() => {
    if (isActive) {
      initializeMediaStream();
    }
  }, [isActive, initializeMediaStream]);

  // ë¶„ì„ ë£¨í”„ ì‹œì‘/ì¤‘ì§€
  useEffect(() => {
    let isActive = isAnalyzing;
    
    if (isActive) {
      const runAnalysis = () => {
        if (!isActive) return;
        
        let currentVoiceVAD: VADScore = { valence: 0.5, arousal: 0.5, dominance: 0.5 };
        let currentFacialVAD: VADScore = { valence: 0.5, arousal: 0.5, dominance: 0.5 };

        // ìŒì„± ë¶„ì„
        if (analyserRef.current && isAudioOn) {
          const bufferLength = analyserRef.current.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          const timeDataArray = new Uint8Array(bufferLength);

          analyserRef.current.getByteFrequencyData(dataArray);
          analyserRef.current.getByteTimeDomainData(timeDataArray);

          const volumeLevel = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength / 255;
          const pitchValue = calculatePitch(timeDataArray);
          const rateValue = calculateSpeechRate(timeDataArray);

          currentVoiceVAD = calculateVoiceVAD(volumeLevel, pitchValue, rateValue);
        }

        // í‘œì • ë¶„ì„ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)
        if (isVideoOn) {
          currentFacialVAD = calculateFacialVAD();
        }

        // í†µí•© VAD ì ìˆ˜ ê³„ì‚°
        const integratedVAD = calculateIntegratedVAD(currentFacialVAD, currentVoiceVAD);
        const emotion = vadToEmotion(integratedVAD);
        const confidenceScore = calculateConfidence(currentFacialVAD, currentVoiceVAD);

        const emotionAnalysis: EmotionAnalysis = {
          id: `realtime_${Date.now()}`,
          userId: 'user123',
          timestamp: new Date().toISOString(),
          vadScore: integratedVAD,
          emotion,
          confidence: confidenceScore,
          mediaType: 'realtime',
          cbtFeedback: {
            cognitiveDistortion: 'ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘',
            challenge: 'í˜„ì¬ ê°ì • ìƒíƒœë¥¼ ê´€ì°°í•´ë³´ì„¸ìš”',
            alternative: 'ê°ì • ë³€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°›ì•„ë“¤ì´ì„¸ìš”',
            actionPlan: 'ì •ê¸°ì ì¸ ê°ì • ì²´í¬ë¥¼ í•´ë³´ì„¸ìš”'
          }
        };

        setCurrentEmotion(emotionAnalysis);
        setConfidence(confidenceScore);

        if (onEmotionChange) {
          onEmotionChange(emotionAnalysis);
        }

        if (isActive) {
          animationFrameRef.current = requestAnimationFrame(runAnalysis);
        }
      };

      runAnalysis();
    } else if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }

    return () => {
      isActive = false;
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [isAnalyzing, isAudioOn, isVideoOn, onEmotionChange]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        try {
          audioContextRef.current.close();
        } catch (error) {
          console.warn('AudioContext already closed:', error);
        }
      }
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [stream]);

  if (!isActive) {
    return null;
  }

  return (
    <div className="relative w-full h-full bg-black rounded-lg overflow-hidden">
      {/* ë¹„ë””ì˜¤ ì˜ì—­ */}
      <div className="relative w-full h-full">
        <video
          ref={videoRef}
          className="w-full h-full object-cover"
          autoPlay
          muted
          playsInline
        />
        <canvas
          ref={canvasRef}
          className="absolute inset-0 w-full h-full pointer-events-none"
        />
        
        {/* ì˜¤ë²„ë ˆì´ ì •ë³´ */}
        <div className="absolute top-4 left-4 bg-black bg-opacity-50 text-white px-3 py-2 rounded-lg">
          <div className="flex items-center space-x-2">
            <Activity className="w-4 h-4" />
            <span className="text-sm">
              {isAnalyzing ? 'ì‹¤ì‹œê°„ ë¶„ì„ ì¤‘' : 'ëŒ€ê¸° ì¤‘'}
            </span>
          </div>
        </div>

        {/* í˜„ì¬ ê°ì • í‘œì‹œ */}
        {currentEmotion && (
          <div className="absolute top-4 right-4 bg-black bg-opacity-50 text-white px-3 py-2 rounded-lg">
            <div className="flex items-center space-x-2">
              <span className="text-lg">{emotionEmojis[currentEmotion.emotion as keyof typeof emotionEmojis] || 'ğŸ˜'}</span>
              <div className="text-sm">
                <div>ì‹ ë¢°ë„: {Math.round(confidence * 100)}%</div>
                <div className="text-xs text-gray-300">
                  V: {Math.round(currentEmotion.vadScore.valence * 100)}% 
                  A: {Math.round(currentEmotion.vadScore.arousal * 100)}% 
                  D: {Math.round(currentEmotion.vadScore.dominance * 100)}%
                </div>
              </div>
            </div>
          </div>
        )}

        {/* ì—ëŸ¬ ë©”ì‹œì§€ */}
        {error && (
          <div className="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 bg-red-500 text-white px-4 py-2 rounded-lg">
            {error}
          </div>
        )}

        {/* ì»¨íŠ¸ë¡¤ ë°” */}
        <div className="absolute bottom-4 left-1/2 transform -translate-x-1/2 bg-black bg-opacity-50 rounded-full px-6 py-3">
          <div className="flex items-center space-x-4">
            {/* ë¶„ì„ í† ê¸€ */}
            <ActionButton
              variant={isAnalyzing ? 'danger' : 'primary'}
              size="sm"
              onClick={toggleAnalysis}
            >
              <Brain className="w-4 h-4" />
            </ActionButton>

            {/* ë¹„ë””ì˜¤ í† ê¸€ */}
            <ActionButton
              variant={isVideoOn ? 'secondary' : 'danger'}
              size="sm"
              onClick={toggleVideo}
            >
              {isVideoOn ? <Video className="w-4 h-4" /> : <VideoOff className="w-4 h-4" />}
            </ActionButton>

            {/* ì˜¤ë””ì˜¤ í† ê¸€ */}
            <ActionButton
              variant={isAudioOn ? 'secondary' : 'danger'}
              size="sm"
              onClick={toggleAudio}
            >
              {isAudioOn ? <Mic className="w-4 h-4" /> : <MicOff className="w-4 h-4" />}
            </ActionButton>

            {/* ì „ì²´í™”ë©´ í† ê¸€ */}
            <ActionButton
              variant="secondary"
              size="sm"
              onClick={toggleFullscreen}
            >
              {isFullscreen ? <Minimize2 className="w-4 h-4" /> : <Maximize2 className="w-4 h-4" />}
            </ActionButton>

            {/* í†µí™” ì¢…ë£Œ */}
            <ActionButton
              variant="danger"
              size="sm"
              onClick={endCall}
            >
              <PhoneOff className="w-4 h-4" />
            </ActionButton>
          </div>
        </div>
      </div>
    </div>
  );
} 